#!/usr/bin/python

"""
make sure we have the thumb+large sizes of all /my/pic/digicam jpgs
ready for serving. Photo site can make its own if any are missing, but
that will be slower.

this should be run after new photos are imported, and maybe every
night to catch strays.

it would be faster to make the 75px images out of the 600px
images. photos.py should just remember and use its last one resize,
and then we can ask for 600 followed by 75.

testing? use this to remove the last 24h of thumbs:
  find /my/pic/~thumb -mtime -1 -type f -delete
"""

import subprocess, random, time, optparse, logging
from photos import justCache
from urls import photoUri
from picdirs import picSubDirs

parser = optparse.OptionParser()
parser.add_option("--quick", action="store_true", help="only scan a few files")
parser.add_option("-d", action="store_true", help="debug logs")
parser.add_option("--grid", action="store_true", help="run resizes as jobs on SGE grid")
opts, args = parser.parse_args()

log = logging.getLogger()
logging.basicConfig(level=logging.DEBUG if opts.d else logging.INFO)

cmd = ["/usr/bin/find"]
cmd += picSubDirs(quick=opts.quick)
cmd += "-regextype posix-egrep -name .xvpics -prune -type f -o".split()
cmd += ['(', '-iregex', r'.*\.(jpg|jpeg|png|tif|tiff)', ')']
log.debug(repr(cmd))
files = [f.strip() for f in subprocess.Popen(cmd,
                 stdout=subprocess.PIPE).communicate()[0].splitlines()]

reportStep = 300
lastReportStart = time.time()
for i, filename in enumerate(files):
    log.debug("file %s: %s", i, filename)
    try:
        justCache(photoUri(filename),
                  grid=opts.grid, gridLogDir='/my/log/grid',
                  sizes=[75,250,600])
    except IOError, e:
        log.error("  failed: %s", e)
        
    if (i + 1) % reportStep == 0:
        perFile = (time.time() - lastReportStart) / reportStep
        lastReportStart = time.time()
        log.info("finished %s of %s, est %s min left" % (
            i, len(files),
            round((len(files) - i) * perFile / 60)))
